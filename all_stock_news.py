import requests
import os
import sys
import pandas as pd
import time
from news_and_halts import make_embed_from_news_item, is_valid_news_item, post_webhook_embeds
from cad_tickers.news import scrap_news_for_ticker
from concurrent.futures import ThreadPoolExecutor

# the setrecursionlimit function is 
# used to modify the default recursion 
# limit set by python. Using this,  
# we can increase the recursion limit 
# to satisfy our needs 
sys.setrecursionlimit(10**6) 

def download_csvs():
  # check if files exists
  # if not download all the csvs from my other github repo
  # eventually I have to grab the latest autogenerated csvs
  # could just duplicate the code over
  url = 'https://friendlyuser.github.io/cad_tickers_list/8/cse_8_2020.csv'
  r = requests.get(url, allow_redirects=True)
  cse_file = 'cse.csv'
  if os.path.exists(cse_file) == False :
    with open(cse_file, 'wb') as file_:
      file_.write(r.content)

  url = 'https://friendlyuser.github.io/cad_tickers_list/8/tsx_8_2020.csv'
  r = requests.get(url, allow_redirects=True)
  tsx_file = 'tsx.csv'
  if os.path.exists(tsx_file) == False :
    with open(tsx_file, 'wb') as file_:
      file_.write(r.content)
  pass

def get_tickers():
  # grab tsx data
  tsx_df = pd.read_csv('tsx.csv')
  tsx_df = tsx_df[['Ex.', 'Ticker']]
  ytickers_series = tsx_df.apply(tsx_ticker_to_yahoo, axis=1)
  ytickers_series = ytickers_series.drop_duplicates(keep='last')
  tsx_tickers = ytickers_series.tolist()

  cse_df = pd.read_csv('cse.csv')
  cse_df = cse_df[['Symbol']]
  ytickers_series = cse_df.apply(cse_ticker_to_yahoo, axis=1)
  ytickers_series = ytickers_series.drop_duplicates(keep='last')
  cse_tickers = ytickers_series.tolist()

  ytickers = [*tsx_tickers, *cse_tickers]
  return ytickers

def cse_ticker_to_yahoo(row: pd.Series)-> str:
  ticker = row['Symbol']
  return f"{ticker}.CN"

def tsx_ticker_to_yahoo(row: pd.Series)-> str:
  """
    Parameters:
      ticker: ticker from pandas dataframe from cad_tickers
      exchange: what exchange the ticker is for
    Returns:
  """
  ticker = row['Ticker']
  exchange = row['Ex.']
  # 1min, 5min, 15min, 30min, 60min, daily, weekly, monthly
  switcher = {
    "TSXV":   "V",
    "TSX":     "TO"
  }
  yahoo_ex = switcher.get(exchange, "TSXV")
  return f"{ticker}.{yahoo_ex}"

if __name__ == "__main__":
  # Grab news for my stocks
  assert sys.version_info >= (3, 6)
  # grabbing all news for all stocks will be done in another script
  # no need to publish the results to github pages
  download_csvs()
  tickers = get_tickers()
  with ThreadPoolExecutor(max_workers=16) as tpe:
    try:
      iterables = tpe.map(scrap_news_for_ticker, tickers)
    except Exception as e:
      print(e)
  raw_news = list(iterables)
  # flatten list
  flatten = lambda l: [item for sublist in l for item in sublist]
  flat_news = flatten(raw_news)
  # remove empty news articles
  valid_news = [i for i in flat_news if is_valid_news_item(i)]
  news_df = pd.DataFrame(valid_news)

  # get old news df from file
  fnews_file = 'full_news.csv'
  if os.path.exists(fnews_file):
    old_news_df = pd.read_csv(fnews_file)
  else:
    old_news_df = pd.DataFrame()
  updated_news_df = pd.concat([old_news_df, news_df]) \
    .drop_duplicates(subset=['link_href', 'link_text', 'ticker'], keep='first') \
    .reset_index(drop=True)

  if updated_news_df.empty == False:
    for index, row in updated_news_df.iterrows():
      # Need to chunk 10 messages, into one
      embeds = make_embed_from_news_item(row)
      # But a channel has a 30 msg/60 sec limit for webhooks
      time.sleep(2)
      post_webhook_embeds(embeds)

  updated_news_df.to_csv(fnews_file)